--> Folder Structure
spring-batch-priority-processing/
â”œâ”€â”€ pom.xml
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ main
â”‚   â”‚   â”œâ”€â”€ java
â”‚   â”‚   â”‚   â””â”€â”€ com
â”‚   â”‚   â”‚       â””â”€â”€ example
â”‚   â”‚   â”‚           â”œâ”€â”€ BatchApplication.java
â”‚   â”‚   â”‚           â”œâ”€â”€ config
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ BatchConfig.java
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ PriorityFilePartitioner.java
â”‚   â”‚   â”‚           â”œâ”€â”€ executor
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ AdaptiveThreadPoolTaskExecutor.java
â”‚   â”‚   â”‚           â”œâ”€â”€ monitor
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ ResourceMonitor.java
â”‚   â”‚   â”‚           â”œâ”€â”€ listener
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ JobCompletionNotificationListener.java
â”‚   â”‚   â”‚           â”œâ”€â”€ model
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ MyRecord.java
â”‚   â”‚   â”‚           â”œâ”€â”€ processor
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ MyProcessor.java
â”‚   â”‚   â”‚           â”œâ”€â”€ reader
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ MyFieldSetMapper.java
â”‚   â”‚   â”‚           â””â”€â”€ writer
â”‚   â”‚   â”‚               â””â”€â”€ MyWriter.java
â”‚   â”‚   â””â”€â”€ resources
â”‚   â”‚       â”œâ”€â”€ application.properties
â”‚   â”‚       â””â”€â”€ input
â”‚   â”‚           â”œâ”€â”€ file-priority_1.csv
â”‚   â”‚           â””â”€â”€ file-priority_2.csv


--> pom.xml

<?xml version="1.0" encoding="UTF-8"?> <!-- XML document declaration -->
<project xmlns="http://maven.apache.org/POM/4.0.0"  <!-- Declare Maven POM XML namespace -->
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  <!-- XML schema instance namespace -->
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0  <!-- POM schema location -->
                             https://maven.apache.org/xsd/maven-4.0.0.xsd"> <!-- Link to schema file -->

    <modelVersion>4.0.0</modelVersion> <!-- Maven model version -->

    <groupId>com.example</groupId> <!-- Maven group (company/project) -->
    <artifactId>spring-batch-priority-processing</artifactId> <!-- Project artifact id -->
    <version>1.0.0</version> <!-- Project version -->
    <packaging>jar</packaging> <!-- Output as JAR -->
    <name>Spring Batch Priority Processing</name> <!-- Human-readable name -->

    <properties>
        <java.version>17</java.version> <!-- Compile using Java 17 -->
        <spring.boot.version>2.7.18</spring.boot.version> <!-- Spring Boot version -->
    </properties>

    <dependencies>
        <dependency> <!-- Spring Batch core with Boot auto-config -->
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-batch</artifactId>
        </dependency>
        <dependency> <!-- JDBC for job repository metadata storage -->
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-jdbc</artifactId>
        </dependency>
        <dependency> <!-- Retry/backoff utilities -->
            <groupId>org.springframework.retry</groupId>
            <artifactId>spring-retry</artifactId>
        </dependency>
        <dependency> <!-- In-memory H2 DB used for job metadata -->
            <groupId>com.h2database</groupId>
            <artifactId>h2</artifactId>
            <scope>runtime</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin> <!-- Enables mvn spring-boot:run and packaging as bootable JAR -->
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>


--> BatchApplication.java

package com.example; // Declare package for main application class

import org.springframework.boot.SpringApplication; // Import Spring Boot launcher
import org.springframework.boot.autoconfigure.SpringBootApplication; // Import auto-config annotation

@SpringBootApplication // Marks this as main Spring Boot app + enables component scanning & auto-config
public class BatchApplication { // Main application class
    public static void main(String[] args) { // Java entry point
        SpringApplication.run(BatchApplication.class, args); // Launch Spring Boot (which also launches Spring Batch job)
    }
}

--> executor/AdaptiveThreadPoolTaskExecutor.java

package com.example.executor; // Package for executor classes

import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor; // Import Spring's thread pool executor

/**
 * Custom ThreadPoolTaskExecutor that can be resized dynamically at runtime.
 */
public class AdaptiveThreadPoolTaskExecutor extends ThreadPoolTaskExecutor { // Extend base executor

    // Thread-safe method to adjust pool sizes during execution
    public synchronized void adjustPoolSizes(int newCoreSize, int newMaxSize) {
        System.out.println("ðŸ”§ Adjusting thread pool: core=" + newCoreSize + ", max=" + newMaxSize); // Log adjustment
        setCorePoolSize(newCoreSize); // Update core size
        setMaxPoolSize(newMaxSize);   // Update max size
    }
}


--> monitor/ResourceMonitor.java

package com.example.monitor; // Package for monitoring classes

import com.example.executor.AdaptiveThreadPoolTaskExecutor; // Import our adaptive executor
import org.springframework.stereotype.Component; // Marks class as Spring bean

import javax.annotation.PostConstruct; // Lifecycle annotation to run method after bean init
import java.lang.management.ManagementFactory; // Access to JVM and OS metrics
import java.lang.management.MemoryMXBean; // Memory metrics interface
import java.lang.management.OperatingSystemMXBean; // OS metrics interface
import java.util.concurrent.atomic.AtomicInteger; // Thread-safe integer for dynamic chunk size

/**
 * Monitors CPU and memory usage, adjusting the executor pool sizes and chunk sizes at runtime.
 */
@Component
public class ResourceMonitor {

    private final AdaptiveThreadPoolTaskExecutor executor; // Reference to executor to adjust it
    private final AtomicInteger dynamicChunkSize = new AtomicInteger(100); // Holds live chunk size

    public ResourceMonitor(AdaptiveThreadPoolTaskExecutor executor) { // Inject executor
        this.executor = executor;
    }

    @PostConstruct // Run once after bean creation
    public void startMonitoring() {
        Thread monitorThread = new Thread(() -> { // Create background monitoring loop
            OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean(); // OS metrics
            MemoryMXBean memBean = ManagementFactory.getMemoryMXBean(); // Memory metrics
            while (true) { // Continuous loop
                try {
                    double cpuLoad = 0; // Default CPU load
                    try {
                        cpuLoad = (double) osBean.getClass() // Use reflection for vendor independence
                                .getMethod("getSystemCpuLoad")
                                .invoke(osBean);
                    } catch (Exception ignore) {}

                    long usedMem = memBean.getHeapMemoryUsage().getUsed(); // Used memory
                    long maxMem = memBean.getHeapMemoryUsage().getMax(); // Max heap
                    double memUsage = (double) usedMem / maxMem; // Usage ratio

                    if (cpuLoad < 0.5 && memUsage < 0.7) { // Plenty of resources
                        executor.adjustPoolSizes(6, 12); // Scale up threads
                        dynamicChunkSize.set(200); // Bigger chunks
                    } else if (cpuLoad > 0.8 || memUsage > 0.9) { // Heavy load
                        executor.adjustPoolSizes(2, 4); // Scale down threads
                        dynamicChunkSize.set(50); // Smaller chunks
                    } else { // Moderate load
                        executor.adjustPoolSizes(4, 8); // Default threads
                        dynamicChunkSize.set(100); // Default chunk size
                    }

                    Thread.sleep(5000); // Wait 5s before rechecking
                } catch (InterruptedException e) {
                    return; // Exit loop if interrupted
                }
            }
        }, "resource-monitor"); // Thread name
        monitorThread.setDaemon(true); // Doesn't block JVM shutdown
        monitorThread.start(); // Kick off monitoring
    }

    public int getDynamicChunkSize() { // Getter for current chunk size
        return dynamicChunkSize.get();
    }
}


--> config/BatchConfig.java


package com.example.config; // Batch configuration package

import com.example.executor.AdaptiveThreadPoolTaskExecutor; // Adaptive executor
import com.example.listener.JobCompletionNotificationListener; // Job listener
import com.example.model.MyRecord; // Model
import com.example.monitor.ResourceMonitor; // Resource monitor
import com.example.processor.MyProcessor; // Processor
import com.example.reader.MyFieldSetMapper; // Mapper
import com.example.writer.MyWriter; // Writer
import org.springframework.batch.core.Job; // Job interface
import org.springframework.batch.core.Step; // Step interface
import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing; // Enable batch infra
import org.springframework.batch.core.configuration.annotation.JobBuilderFactory; // Job builder
import org.springframework.batch.core.configuration.annotation.StepBuilderFactory; // Step builder
import org.springframework.batch.core.partition.support.Partitioner; // Partitioner API
import org.springframework.batch.item.file.FlatFileItemReader; // File reader
import org.springframework.batch.item.file.mapping.DefaultLineMapper; // Maps lines to objects
import org.springframework.batch.item.file.transform.DelimitedLineTokenizer; // Tokenizer for CSV
import org.springframework.batch.retry.backoff.ExponentialBackOffPolicy; // Retry backoff
import org.springframework.beans.factory.annotation.Autowired; // Injection
import org.springframework.beans.factory.annotation.Value; // Get step execution context values
import org.springframework.context.annotation.Bean; // Define bean
import org.springframework.context.annotation.Configuration; // Config class
import org.springframework.core.io.FileSystemResource; // Resource for files
import org.springframework.batch.core.configuration.annotation.StepScope; // Step-scoped bean

@Configuration // Marks as Spring config
@EnableBatchProcessing // Enable Spring Batch infra
public class BatchConfig {

    @Autowired // Inject job builder factory
    private JobBuilderFactory jobBuilderFactory;
    @Autowired // Inject step builder factory
    private StepBuilderFactory stepBuilderFactory;
    @Autowired // Inject resource monitor
    private ResourceMonitor resourceMonitor;

    @Bean // Define adaptive executor bean
    public AdaptiveThreadPoolTaskExecutor taskExecutor() {
        AdaptiveThreadPoolTaskExecutor executor = new AdaptiveThreadPoolTaskExecutor(); // Create
        executor.setCorePoolSize(4); // Default core size
        executor.setMaxPoolSize(8); // Default max size
        executor.setQueueCapacity(2); // Limited queue
        executor.setThreadNamePrefix("batch-thread-"); // Thread naming
        executor.initialize(); // Init executor
        return executor; // Return as bean
    }

    @Bean // Define job
    public Job job(JobCompletionNotificationListener listener, Step partitionStep) {
        return jobBuilderFactory.get("adaptiveJob") // Job name
                .listener(listener) // Add listener
                .flow(partitionStep) // Start with partition step
                .end() // End job building
                .build(); // Build job
    }

    @Bean // Partition step bean
    public Step partitionStep(AdaptiveThreadPoolTaskExecutor executor, Step workerStep, Partitioner filePartitioner) {
        return stepBuilderFactory.get("partitionStep")
                .partitioner("workerStep", filePartitioner) // Partition logic
                .step(workerStep) // Step to execute for each partition
                .taskExecutor(executor) // Parallel execution
                .gridSize(4) // Partitions at a time
                .build();
    }

    @Bean // Worker step bean
    public Step workerStep(FlatFileItemReader<MyRecord> reader, MyProcessor processor, MyWriter writer) {
        return stepBuilderFactory.get("workerStep")
                .<MyRecord, MyRecord>chunk(resourceMonitor.getDynamicChunkSize()) // Dynamic chunk size
                .reader(reader) // Reader
                .processor(processor) // Processor
                .writer(writer) // Writer
                .faultTolerant() // Enable retry
                .retry(Exception.class) // Retry on any exception
                .retryLimit(5) // Max retries
                .backOffPolicy(backOffPolicy()) // Exponential backoff
                .build();
    }

    @Bean // File partitioner
    public Partitioner filePartitioner() {
        return new PriorityFilePartitioner("src/main/resources/input"); // Partition per file by priority
    }

    @Bean // Retry backoff bean
    public ExponentialBackOffPolicy backOffPolicy() {
        ExponentialBackOffPolicy policy = new ExponentialBackOffPolicy(); // Create
        policy.setInitialInterval(1000); // Start delay
        policy.setMultiplier(2.0); // Double each time
        policy.setMaxInterval(8000); // Max delay
        return policy;
    }

    @Bean // Reader bean
    @StepScope // Each step has its own instance
    public FlatFileItemReader<MyRecord> itemReader(@Value("#{stepExecutionContext['fileName']}") String fileName) {
        FlatFileItemReader<MyRecord> reader = new FlatFileItemReader<>(); // Create reader
        reader.setResource(new FileSystemResource(fileName)); // Bind to file from context
        reader.setLineMapper(new DefaultLineMapper<>() {{ // Map line to object
            setLineTokenizer(new DelimitedLineTokenizer(",") {{ // Tokenize CSV
                setNames("field1", "field2", "field3"); // Columns
            }});
            setFieldSetMapper(new MyFieldSetMapper()); // Map to MyRecord
        }});
        return reader;
    }
}


--> config/PriorityFilePartitioner.java

package com.example.config; // Package

import org.springframework.batch.core.partition.support.Partitioner; // Interface
import org.springframework.batch.item.ExecutionContext; // Step context

import java.io.File; // Files
import java.util.*; // Util types
import java.util.stream.Collectors; // Stream API

public class PriorityFilePartitioner implements Partitioner {

    private final String inputDir; // Directory

    public PriorityFilePartitioner(String inputDir) { // Set directory
        this.inputDir = inputDir;
    }

    @Override
    public Map<String, ExecutionContext> partition(int gridSize) {
        Map<String, ExecutionContext> partitions = new HashMap<>(); // Map partitions
        File[] files = new File(inputDir).listFiles(); // All files
        if (files == null) throw new IllegalStateException("No files found"); // Guard

        List<File> sortedFiles = Arrays.stream(files)
                .sorted(Comparator.comparingInt(this::extractPriority)) // Sort by priority
                .collect(Collectors.toList());

        int partitionNum = 0;
        for (File file : sortedFiles) {
            ExecutionContext ctx = new ExecutionContext(); // Context per file
            ctx.putString("fileName", file.getAbsolutePath()); // Put file path
            partitions.put("partition" + partitionNum, ctx); // Add to map
            partitionNum++;
        }
        return partitions;
    }

    private int extractPriority(File file) { // Priority from name
        String name = file.getName();
        try {
            int pIndex = name.indexOf("priority_");
            if (pIndex >= 0) {
                String numPart = name.substring(pIndex + 9).split("\\.")[0];
                return Integer.parseInt(numPart);
            }
        } catch (Exception ignored) {}
        return Integer.MAX_VALUE; // Default (lowest priority)
    }
}


--> listener/JobCompletionNotificationListener.java


package com.example.listener; // Package

import org.springframework.batch.core.JobExecution; // Job execution metadata
import org.springframework.batch.core.listener.JobExecutionListenerSupport; // Base listener
import org.springframework.stereotype.Component; // Bean

@Component // Spring detects this as a bean
public class JobCompletionNotificationListener extends JobExecutionListenerSupport {
    @Override
    public void afterJob(JobExecution jobExecution) { // After job ends
        System.out.println("=== JOB FINISHED with status: " + jobExecution.getStatus() + " ===");
    }
}


--> model/MyRecord.java

package com.example.model; // Package

public class MyRecord {
    private String field1; // Field 1
    private String field2; // Field 2
    private String field3; // Field 3

    public String getField1() { return field1; } // Getter 1
    public void setField1(String field1) { this.field1 = field1; } // Setter 1
    public String getField2() { return field2; } // Getter 2
    public void setField2(String field2) { this.field2 = field2; } // Setter 2
    public String getField3() { return field3; } // Getter 3
    public void setField3(String field3) { this.field3 = field3; } // Setter 3

    @Override
    public String toString() { // For logging
        return "MyRecord{" +
                "field1='" + field1 + '\'' +
                ", field2='" + field2 + '\'' +
                ", field3='" + field3 + '\'' +
                '}';
    }
}

--> reader/MyFieldSetMapper.java

package com.example.reader; // Package

import com.example.model.MyRecord; // Model
import org.springframework.batch.item.file.mapping.FieldSetMapper; // Mapper API
import org.springframework.batch.item.file.transform.FieldSet; // FieldSet

public class MyFieldSetMapper implements FieldSetMapper<MyRecord> {
    @Override
    public MyRecord mapFieldSet(FieldSet fieldSet) { // Convert FieldSet to object
        MyRecord record = new MyRecord();
        record.setField1(fieldSet.readString("field1"));
        record.setField2(fieldSet.readString("field2"));
        record.setField3(fieldSet.readString("field3"));
        return record;
    }
}


--> processor/MyProcessor.java

package com.example.processor; // Package

import com.example.model.MyRecord; // Model
import org.springframework.batch.item.ItemProcessor; // Processor API

public class MyProcessor implements ItemProcessor<MyRecord, MyRecord> {
    @Override
    public MyRecord process(MyRecord item) { // Process record
        return item; // No change
    }
}


--> writer/MyWriter.java

package com.example.writer; // Package

import com.example.model.MyRecord; // Model
import org.springframework.batch.item.ItemWriter; // Writer API
import java.util.List; // List

public class MyWriter implements ItemWriter<MyRecord> {
    @Override
    public void write(List<? extends MyRecord> items) { // Write chunk
        System.out.println("Writing chunk: " + items.size());
        for (MyRecord record : items) {
            System.out.println(record);
        }
    }
}


--> resources/application.properties


spring.batch.initialize-schema=always # Initialize metadata tables each run
spring.main.allow-bean-definition-overriding=true # Allow bean overriding
logging.level.org.springframework.batch=INFO # Batch logging




